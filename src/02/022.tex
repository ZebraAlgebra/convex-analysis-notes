\section{Smoothness Conditions and Convexity}
\label{sect:022}

\paragraph{}Let us fix a proper scalar function $f$ on $\mathbb{R}^m$ with $\operatorname{dom}(f)=X$. We aim to illustrate several results relating smoothness conditions (such as continuity, 1st, 2nd differentaibility) and convexity. First we define a common construction:

\begin{defn}[Slope of Secant, Directional Derivatives]\label{defn:022-secant-slope}
	Fix a point $x\in X$, a direction $d\in \mathbb{R}^m$, a scalar $\gamma \in \mathbb{R}_{++}$, we define the slope of secant $S(x; d; \alpha)=\alpha^{-1}(f(x+\alpha d)-f(x))$, and the directional derivative $f'(x; d)=\lim_{\alpha \to 0^+}S(x;d; \alpha )$ provided that this limit exists.
\end{defn}

\begin{rmrk}Suppose $f$ is convex, given fixed $x, d$, $S(f,x; d; \alpha)$ is increasing in $\alpha$ (using the convention $\infty\leq \infty$), so it makes sense to talk about $\lim_{\alpha \to0^+},\lim_{\alpha \to\infty}$ of $S(f,x; d; \alpha )$ (might go to $\pm\infty$), which corresponds to $\inf$, $\sup$ of $S(f,x; d; \alpha )$ over $\alpha \in \mathbb{R}_{++}$. Taking $\lim_{\alpha \to 0^+}$ gives the directional derivative, while taking $\lim_{\alpha \to +\infty}$ gives "recession functions" (see \Cref{sect:024}) in some cases.
\end{rmrk}

\begin{prop}[First and Second Order Condition for Convexity]
	\label{prop:022-ordered-conditions-convexity}
	Suppose $\emptyset\neq C\subset U\subset X$ with $U$ open, $C$ convex,
	\begin{enumerate}[label=(\alph*)]
		\item If $f\in\mathscr{C}^1(U)$, then $f$ is convex on $C$ iff $f(y)\geq f(x)+\nabla f(x)^T(y-x)$ for $x,y\in C$.
		\item If $f\in\mathscr{C}^2(U)$, then $f$ is convex on $C$ if $f(z)$ is positive semidefinite on $C$. "Only if" holds if $C$ is open.
	\end{enumerate}
\end{prop}

\begin{proof}[Proof of (a)]
	Fix $x,y\in C,\alpha \in [0, 1]$. For "if", let $z=I(x,y; \alpha )$. We have:
	\[
		I(f(x),f(y);\alpha )  \geq I(f(z)+\nabla f(z)^T(x-z),f(z)+\nabla f(z)^T(y-z);\alpha ) = f(z)
	\]
	The "only if" part is the same as $\lim_{\alpha \to0^+}S(f,x; y-x; \alpha )\leq S(f,x; y-x; 1)$, which is given above.
\end{proof}
\begin{proof}[Proof of (b)]
	Fix $x,y\in C,\alpha \in [0, 1]$. For "if", we have by mean value theorem that:
	\[
		f(y)=f(x)+\nabla f(x)^T(y-x)+\frac{1}{2}(y-x)^T\nabla^2f(I(x,y,\alpha ))(y-x)\geq f(x)+\nabla f(x)^T(y-x)
	\]
	for some $\alpha \in[0,1]$. The "Only if" part is similar: if $\nabla^2 f$ is not positive semidefinite on some $x\in C$, take some $z$ so that $B(x;\|z\|)\subset C$, $z^T\nabla^2f(I(x, x+z;\alpha))z<0$ for $\alpha\in[0,1]$, then mean value theorem says $f(x+z)<f(x)+\nabla f(x)^Tz$, contradiction.
\end{proof}

\paragraph{}A remarkable property of convex functions is that it has some continuity properties.

\begin{prop}[Continuity Theorem]
	\label{prop:022-continuity}
	Suppose $f$ is proper, convex, then $f$ is continuous on $\operatorname{ri}(X)$.
\end{prop}

\begin{proof}
	Let $C=\operatorname{ri}(X)$. It suffices to show under the assumptions $\dim C=m$, $E:=[-1,1]^n\subset C$ that $f$ is continuous at $0$. By convexity, $f$ is bounded on $E=\operatorname{conv}(\{-1,1\}^n)$. Take $E\smallsetminus\{0\}\supset \{x_i\}_i\to 0$, and take:
	\[
		y_i=\frac{x_i}{\|x_i\|_\infty}\in \operatorname{bdy}(E),\;
		z_i:=-y_i\in \operatorname{bdy}(E)
	\]
	so we have $0\in I(x_i,z_i;[0,1])$, $x_i\in I(0,y_i;[0,1])$. Now we estimate $f(0)$: Firstly,
	\[
		f(0)=f\left(I\left(z_i,x_i;\frac{1}{\|x_i\|_\infty+1}\right)\right)\leq I\left(f(z_i),f(x_i);\frac{1}{\|x_i\|_\infty+1}\right)
	\]
	Since $\{f(z_i)\}_i$ is bounded, and $\|x_i\|_\infty\to0$, we get $f(0)\leq \underset{i}{\operatorname{liminf}}f(x_i)$. Nextly:
	\[
		f(x_i)=f(I(y_i, 0;\|x\|_\infty))\leq I(f(y_i),f(0);\|x\|_\infty)
	\]
	Again, since $\{f(y_i)\}_i$ is bounded, and $\|x_i\|_\infty\to0$, we get $\underset{i}{\operatorname{limsup}}f(x_i)\leq f(0)$.
\end{proof}

\begin{coro}
	\label{coro:022-continuity-real-valued}
	Suppose $f$ is convex and $X=\mathbb{R}^m$, then $f\in \mathscr{C}^0(\mathbb{R}^m)$.
\end{coro}

\begin{coro}[Line Continuity Theorem]
	\label{coro:022-line-continuity}
	Suppose $m=1$, $f$ is closed, convex, and real-valued on $C=[0,1]$, then $f\in \mathscr{C}^0(C)$.
\end{coro}

\begin{proof}
	It suffices to show upper-semicontinuity at $0$. For $C\supset (x_i)_i\to 0$, we have:
	\[
		f(x_i)=f(I(0,1;x_i))\leq I(f(0),f(1); x_i)
	\]
	which gives $\underset{i}{\operatorname{limsup}}f(x_i)\leq f(0)$.
\end{proof}

\paragraph{}Suppose $f$ is convex, given $x\in X$, the gradient of $f$ at $x$ might not be well-defined. As a compromise, we can still consider the set of subgradients, which is the set of vectors that has the property given in (a) of \Cref{prop:022-ordered-conditions-convexity}. They are essential in optimization (e.g. subgradient descent).

\begin{defn}[Subgradients]\label{defn:022-subgradients}
	Given convex $f$ and $x\in X$, define $\partial f(x)$ as:
	\[
		\partial f(x) = \{
		y\in \mathbb{R}^m: f(z) - f(x) \geq y^T(z- x)\;\text{ for all }z\in \mathbb{R}^m
		\}
	\]
	called the subgradient of $f$ at $x$. Alternatively, subgradients can be defined via directional derivatives:
	\begin{align*}
		\partial f(x) & = \{y\in \mathbb{R}^m: f(x+d \alpha )-f(x) \geq \alpha y^Td \text{ for all }d \in \mathbb{R}^m, \alpha\in \mathbb{R}_{++} \} \\
		              & = \{y\in \mathbb{R}^m: S(f,x; d; \alpha) \geq y^Td \text{ for all }d \in \mathbb{R}^m, \alpha\in \mathbb{R}_{++} \}          \\
		              & = \{y\in \mathbb{R}^m: f'(x; d) \geq y^T d \text{ for all }d \in \mathbb{R}^m \}
	\end{align*}
	We also define $\partial f(x)=\emptyset$ if $x\notin X$.
\end{defn}

\paragraph{}Properties of subgradients will be established later in \Cref{sect:046}, using MC/MC framework.

