\section{Recession Cones, Linearity Subspace}
\label{sect:013}

\paragraph{}Recession cones, linearity subspaces are used for the study of limiting behaviors of convex sets. Fix a nonempty convex $C\subset \mathbb{R}^m$. In this section, given $d\in \mathbb{R}^m$, write the ray and the line with direction $d$ as:
\[
	\rho_d=I(0,d;\mathbb{R}_{+}),\;\ell_d=I(0,d;\mathbb{R})
\]

\begin{defn}[Recession Cone, Linearity Subspace]
	\label{defn:013-recessioncone}
	Given $d\in \mathbb{R}^m$, we say that $d$ is a direction of recession of $C$ if $C+\rho_d\subset C$.	The set of recession directions of $C$ is the recession cone $R_C$. The linearity subspace of $C$ is given as $L_C:=R_C\cap R_{-C}$; equivalently, $d\in L_C$ iff $C+\ell_d\subset C$.
\end{defn}

\paragraph{}We will see that recession cones are convex cones. Recall the map $\nu:\mathbb{R}^m\smallsetminus\{0\}\to \mathbb{S}^{m-1}$ given by $x\mapsto x/\|x\|$; this map gives a correspondence between subsets of $\mathbb{S}^{m-1}$ and cones in $\mathbb{R}^m$ via direct and inverse images.

\begin{prop}[Yoga of Recession Cones].
	\label{prop:013-yoga-recession}
	\begin{enumerate}[label=(\alph*)]
		\item (Recession Cones are Convex Cones) $R_C$ is a non-empty convex cone.
		\item (Product Rule) Given non-empty convex $\{C_i\}_{i=1}^k$ where $C_i\subset \mathbb{R}^{m_i}$, then $R_{\bigtimes_i^kC_i}=\bigtimes_{i=1}^kR_{C_i}$.
	\end{enumerate}
	Assume $C$ is closed in (c)-(h).
	\begin{enumerate}[label=(\alph*)]
		\setcounter{enumi}{2}
		\item (Closedness Rule) $R_C$ is closed.
		\item (Simplified Recession Direction Criterion) For any $x\in C$, $d\in R_C$ iff $x+\rho_d\subset C$.
		\item (Compactness Rule) $R_C=\{0\}$ iff $C$ is bounded (hence iff $C$ is compact).
		\item (Rint Rule) $R_C=R_{\operatorname{ri}(C)}$.
		\item (Intersection Rule) Given closed convex $\{C_i\}_{i\in I}$ in $\mathbb{R}^m$ with $\bigcap_i{C_i}\neq\emptyset$, $R_{\bigcap_iC_i}=\bigcap_iR_{C_i}$.
		\item (Inverse Image Rule) Given affine $A:\mathbb{R}^m\to \mathbb{R}^n$, compact convex $W\subset \mathbb{R}^n$,\\we have $R_{A^{-1}W\cap C}=\operatorname{ker}(A)\cap R_C$ if $A^{-1}W\cap C\neq\emptyset$.
	\end{enumerate}
\end{prop}

\begin{rmrk}
	A generalization of item (f) is that when $C$ is not necessarily closed, $R_{\operatorname{cl}(C)}=R_{\operatorname{ri}(C)}$, which is a direct consequence of (f) in view of $\operatorname{ri}(\operatorname{cl}(C))=\operatorname{ri}(C)$.
\end{rmrk}

\begin{proof}
	For (a), $R_C$ is closed under non-negative rescaling and contains $0$. For convexity, given $d,d'\in R_C$:
	\[
		C+\rho_{I(d,d';[0,1])}\subset I\left(C+\rho_{d},C+\rho_{d'},[0,1]\right)\subset C
	\]
	(b) is straightforward. For (c), given $R_C\supset \{d_i\}_i\to d\in \operatorname{cl}(R_C)$, $C+\rho_d\subset \operatorname{cl}\left(\bigcup_i\left(C+\rho_{d_i}\right)\right)\subset \operatorname{cl}(C)=C$.\\
	For "if" of (d), for $d\in R_C$, we have: $C+\rho_d=\operatorname{cl}\left(x+\rho_d,C;[0,1]\right)\subset \operatorname{cl}(C)=C$. "Only if" is trivial.\\
	For "only if" of (e), may assume $\dim C=m$, $0\in C$. We use (d) on $0$. Take $C\supset \{x_i\}_i$ with $\|x_i\|\geq i$. Let $d_i:=\nu(x_i)\in \mathbb{S}^{m-1}$; it has some limit point $d\in \mathbb{S}^{m-1}$, so by rechoosing $x_i$, may assume $\{d_i\}_i\to d$. We show that $d\in R_C$. It suffices to show $\{id\}_i\subset C$, but this is by $C\supset\{id_j\}_{j\geq i}^\infty\to id$, as $id_j\in I(0,x_j;[0,1])$ for $j\geq i$. "If" is trivial for (e).\\
	For (f), "$\supset$" is by (d); for "$\subset$", take $x\in \operatorname{ri}(C)$, then given $d\in R_C$, by (b) of \Cref{prop:012-basic-ri}:
	\[
		x+\rho_d = I(x,x+\rho_d;[0, 1))\subset \operatorname{ri}(C)
	\]
	For (g), use (d). For (h), by (g), we only need $R_{A^{-1}W}=\operatorname{ker}(A)$, which is an easy application of (e).
\end{proof}

\begin{prop}[Yoga of Linearity Subspaces].
	\label{prop:013-yoga-linearity}
	\begin{enumerate}[label=(\alph*)]
		\item (Linearity Subspaces are Linear Subspaces) $L_C$ is a vector subspace, hence also closed.
		\item (Product Rule) Given convex $\{C_i\}_{i=1}^k$ where $C_i\subset \mathbb{R}^{m_i}$, write $C=\bigtimes_{i=1}^kC_i$, then $L_C=\bigtimes_{i=1}^kL_{C_i}$.
	\end{enumerate}
	Assume $C$ is closed in (c)-(g).
	\begin{enumerate}[label=(\alph*)]
		\setcounter{enumi}{2}
		\item (Simplified Recession Direction Criterion) For any $x\in C$, $d\in L_C$ iff $x+\ell_d\subset C$.
		\item (Compactness Rule) $L_C=\{0\}$ if $C$ is bounded.
		\item (Rint Rule) $L_C=L_{\operatorname{ri}(C)}$.
		\item (Intersection Rule) Given closed convex $\{C_i\}_{i\in I}$ in $\mathbb{R}^m$ with $\bigcap_i{C_i}\neq\emptyset$, we have $L_{\bigcap_iC_i}=\bigcap_iL_{C_i}$.
		\item (Inverse Image Rule) Given affine $A:\mathbb{R}^m\to \mathbb{R}^n$, compact convex $W\subset \mathbb{R}^n$,\\we have $L_{A^{-1}W\cap C}=\operatorname{ker}(A)\cap L_C$ if $A^{-1}W\cap C\neq\emptyset$.
	\end{enumerate}
\end{prop}

\begin{proof}
	We only show (a); (b)-(g) are direct consequences of (b),(d)-(h) of \Cref{prop:013-yoga-recession} and the definition of $L_C$ as in \Cref{defn:013-recessioncone}. For (a), by $L_C=-L_{-C}$ and (a) of \Cref{prop:013-yoga-recession}:
	\[
		\operatorname{lin}(L_C)=\operatorname{c.cone}(L_C)\subset\operatorname{c.cone}(R_C)\cap \operatorname{c.cone}(R_{-C})=R_C\cap R_{-C}=L_C\qedhere{}
	\]
\end{proof}

\paragraph{}A cool use of linearity subspace is the following decomposition of convex sets into "linear + nonlinear":

\begin{prop}[Decomposition into Linear plus No Linear]
	\label{prop:013-decomposition}
	Let $S\subset L_C$ be a subspace, then:
	\[C=S+(C\cap S^{\perp})\]
\end{prop}

\paragraph{}In particular, $C=L_C+(C\cap L_C^\perp)$ is a decomposition, one with all linearities of $C$ and one with none:
\[
	L_{L_C}=L_C,\; L_{C\cap L_C^\perp}=L_C\cap L_{L_C^\perp}=L_C\cap L_C^\perp=0
\]

\begin{proof}
	It is evident that "$\supset$" holds. For "$\subset$", given $x=s+t\in C$, with $s\in S,t\in S^\perp$, then $t\in C$.
\end{proof}

